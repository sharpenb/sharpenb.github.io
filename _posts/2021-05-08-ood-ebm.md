---
layout: post
title: On Out-of-distribution Detection with Energy-Based Models
subtitle: UDL 2021 (ICML)
cover-img: /assets/img/path.jpg
thumbnail-img: /assets/img/ood-ebm.png
share-img: /assets/img/path.jpg
gh-repo: selflein/EBM-OOD-Detection
gh-badge: [star, fork, follow]
tags: [Uncertainty Estimation, Energy-Based Models]
comments: false
---

This page is about our paper

[**On Out-of-distribution Detection with Energy-Based Models**](https://arxiv.org/pdf/2107.08785.pdf)

by Sven Elflein, Bertrand Charpentier, Daniel Zügner, and Stephan Günnemann
Published at the Uncertainty and Robustness in Deep Learning Wrokshop (UDL - ICML), 2021

**Abstract**

Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks.

**Links**

[Paper](https://arxiv.org/pdf/2107.08785.pdf) | [Github](https://github.com/selflein/EBM-OOD-Detection)
